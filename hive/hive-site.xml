<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?><!-- Licensed 
	to the Apache Software Foundation (ASF) under one or more contributor license 
	agreements. See the NOTICE file distributed with this work for additional 
	information regarding copyright ownership. The ASF licenses this file to 
	You under the Apache License, Version 2.0 (the "License"); you may not use 
	this file except in compliance with the License. You may obtain a copy of 
	the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required 
	by applicable law or agreed to in writing, software distributed under the 
	License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS 
	OF ANY KIND, either express or implied. See the License for the specific 
	language governing permissions and limitations under the License. -->
<configuration>
	<!-- WARNING!!! This file is auto generated for documentation purposes ONLY! -->
	<!-- WARNING!!! Any changes you make to this file will be ignored by Hive. -->
	<!-- WARNING!!! You must make your changes in hive-site.xml instead. -->
	<!-- Hive Execution Parameters -->
	<property>
		<name>javax.jdo.option.ConnectionURL</name>
		<value>jdbc:mysql://192.168.1.82:3306/hive42?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8</value>
		<description>JDBC connect string for a JDBC metastore</description>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionDriverName</name>
		<value>com.mysql.jdbc.Driver</value>
		<description>Driver class name for a JDBC metastore</description>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionUserName</name>
		<value>root</value>
		<description>username to use against metastore database</description>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionPassword</name>
		<value>123456</value>
		<description>password to use against metastore database</description>
	</property>
	 
	<property>
		<name>hive.execution.engine</name>
		<value>spark</value>
	</property>
	<property>
		<name>hive.enable.spark.execution.engine</name>
		<value>true</value>
	</property> 
	<!--其它 <property> <name>mapreduce.input.fileinputformat.split.maxsize</name> 
		<value>750000000</value> </property> <property> <name>hive.vectorized.execution.enabled</name> 
		<value>true</value> </property> <property> <name>hive.cbo.enable</name> <value>true</value> 
		</property> <property> <name>hive.optimize.reducededuplication.min.reducer</name> 
		<value>4</value> </property> <property> <name>hive.optimize.reducededuplication</name> 
		<value>true</value> </property> <property> <name>hive.orc.splits.include.file.footer</name> 
		<value>false</value> </property> <property> <name>hive.merge.mapfiles</name> 
		<value>true</value> </property> <property> <name>hive.merge.sparkfiles</name> 
		<value>false</value> </property> <property> <name>hive.merge.smallfiles.avgsize</name> 
		<value>16000000</value> </property> <property> <name>hive.merge.size.per.task</name> 
		<value>256000000</value> </property> <property> <name>hive.merge.orcfile.stripe.level</name> 
		<value>true</value> </property> <property> <name>hive.auto.convert.join</name> 
		<value>true</value> </property> <property> <name>hive.auto.convert.join.noconditionaltask</name> 
		<value>true</value> </property> <property> <name>hive.auto.convert.join.noconditionaltask.size</name> 
		<value>894435328</value> </property> <property> <name>hive.optimize.bucketmapjoin.sortedmerge</name> 
		<value>false</value> </property> <property> <name>hive.map.aggr.hash.percentmemory</name> 
		<value>0.5</value> </property> <property> <name>hive.map.aggr</name> <value>true</value> 
		</property> <property> <name>hive.optimize.sort.dynamic.partition</name> 
		<value>false</value> </property> <property> <name>hive.stats.autogather</name> 
		<value>true</value> </property> <property> <name>hive.stats.fetch.column.stats</name> 
		<value>true</value> </property> <property> <name>hive.vectorized.execution.reduce.enabled</name> 
		<value>false</value> </property> <property> <name>hive.vectorized.groupby.checkinterval</name> 
		<value>4096</value> </property> <property> <name>hive.vectorized.groupby.flush.percent</name> 
		<value>0.1</value> </property> <property> <name>hive.compute.query.using.stats</name> 
		<value>true</value> </property> <property> <name>hive.limit.pushdown.memory.usage</name> 
		<value>0.4</value> </property> <property> <name>hive.optimize.index.filter</name> 
		<value>true</value> </property> <property> <name>hive.exec.reducers.bytes.per.reducer</name> 
		<value>67108864</value> </property> <property> <name>hive.smbjoin.cache.rows</name> 
		<value>10000</value> </property> <property> <name>hive.exec.orc.default.stripe.size</name> 
		<value>67108864</value> </property> <property> <name>hive.fetch.task.conversion</name> 
		<value>more</value> </property> <property> <name>hive.fetch.task.conversion.threshold</name> 
		<value>1073741824</value> </property> <property> <name>hive.fetch.task.aggr</name> 
		<value>false</value> </property> <property> <name>mapreduce.input.fileinputformat.list-status.num-threads</name> 
		<value>5</value> </property> <property> <name>spark.kryo.referenceTracking</name> 
		<value>false</value> </property> <property> <name>spark.kryo.classesToRegister</name> 
		<value>org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch 
		</value> </property> -->

	<!--sparkcontext -->
	<property>
		<name>spark.master</name>
		<value>yarn-cluster</value>
	</property>
	<property>
		<name>spark.serializer</name>
		<value>org.apache.spark.serializer.KryoSerializer</value>
	</property>
	<property>
		<name>spark.executor.instances</name>
		<value>3</value>
	</property>
	<property>
		<name>spark.executor.cores</name>
		<value>13</value>
	</property>
	<property>
		<name>spark.executor.memory</name>
		<value>8g</value>
	</property>
	<property>
		<name>spark.driver.cores</name>
		<value>2</value>
	</property>
	<property>
		<name>spark.driver.memory</name>
		<value>4096m</value>
	</property>
	<property>
		<name>spark.yarn.queue</name>
		<value>default</value>
	</property>
	<property>
		<name>spark.app.name</name>
		<value>myInceptor</value>
	</property>

	<!--事务相关 -->
	<property>
		<name>hive.support.concurrency</name>
		<value>true</value>
	</property>
	<property>
		<name>hive.enforce.bucketing</name>
		<value>true</value>
	</property>
	<property>
		<name>hive.exec.dynamic.partition.mode</name>
		<value>nonstrict</value>
	</property>
	<property>
		<name>hive.txn.manager</name>
		<value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
	</property>
	<property>
		<name>hive.compactor.initiator.on</name>
		<value>true</value>
	</property>
	<property>
		<name>hive.compactor.worker.threads</name>
		<value>1</value>
	</property>
	<property>
		<name>spark.executor.extraJavaOptions</name>
		<value>-XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
		</value>
	</property>
	<!--jar -->
	<property>
		<name>hive.aux.jars.path</name>
		<value>file:/opt/hive/lib/hive-hbase-handler-2.3.0.jar,file:/opt/hive/lib/phoenix-4.11.0-HBase-1.2-hive.jar,file:/opt/hive/lib/elasticsearch-hadoop-mr-5.5.1.jar,file:/opt/hive/lib/elasticsearch-hadoop-hive-5.5.1.jar,file:/opt/hive/lib/elasticsearch-sql-5.1.2.0.jar</value>
	</property>
	<!--其它 -->
	<property>
		<name>hive.server2.enable.doAs</name>
		<value>false</value>
	</property>
	<property>
		<name>hbase.zookeeper.quorum</name>
		<value>ht05:2181,ht06:2181,ht07:2181</value>
	</property>
	
</configuration>
